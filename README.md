# The Modern Data Stack

> **Transform your CSV data into a queryable, production-ready data lakehouse**

A comprehensive Go-based pipeline that converts CSV files into Apache Iceberg tables with automatic schema detection, powered by DuckDB and Trino.

## ğŸ¯ What This Does

**Input**: Your CSV files  
**Output**: Queryable Iceberg data lakehouse with web-based SQL interface

```
CSV Files â†’ Parquet â†’ Iceberg Tables â†’ Analytics
```

## ğŸš€ Quick Start (2 Minutes)

```bash
# 1. Clone and setup
git clone <your-repo>
cd the-modern-data-stack
just deps

# 2. Add your CSV files
cp your-data.csv data/source/

# 3. Run complete pipeline
just full-workflow

# 4. Query your data
just trino-cli
# Or visit: http://localhost:8080
```

## ğŸ“Š Architecture Overview

```mermaid
graph TB
    A[ğŸ“ CSV Files<br/>data/source/] --> B[ğŸ”„ CSV to Parquet<br/>DuckDB Converter]
    B --> C[ğŸ“¦ Parquet Files<br/>data/parquet/]
    
    C --> D[ğŸ§Š Iceberg Table Creator<br/>DuckDB Schema Inspector]
    D --> E[ğŸ“Š Iceberg Tables<br/>data/iceberg_warehouse/]
    
    F[ğŸ³ Docker Compose] --> G[ğŸ—„ï¸ Iceberg REST Catalog<br/>localhost:8181]
    F --> H[ğŸ” Trino Query Engine<br/>localhost:8080]
    
    G --> E
    H --> E
    
    E --> I[ğŸ“ˆ Analytics & Queries]
    C --> I
    
    I --> J[ğŸ¦† DuckDB<br/>Direct Parquet Access]
    I --> K[ğŸ” Trino Web UI<br/>SQL Interface]
    I --> L[ğŸ“Š Business Intelligence<br/>Dashboards & Reports]
    
    style A fill:#e1f5fe
    style C fill:#f3e5f5
    style E fill:#e8f5e8
    style F fill:#fff3e0
    style I fill:#fce4ec
```

The pipeline automatically transforms your data through these stages:

**Stage 1: CSV â†’ Parquet**
- Discovers all CSV files in `data/source/`
- Converts to efficient Parquet format using DuckDB
- Preserves data types and handles malformed files

**Stage 2: Parquet â†’ Iceberg**
- Reads actual Parquet schemas using DuckDB
- Creates Iceberg tables with proper column types
- Preserves nullability and field metadata

**Stage 3: Query & Analytics**
- Trino web interface for SQL queries
- DuckDB for high-performance analytics
- Compatible with Spark, dbt, and BI tools

## ğŸ³ Services

| Service | URL | Purpose |
|---------|-----|---------|
| **Trino Web UI** | http://localhost:8080 | Interactive SQL queries |
| **Iceberg REST Catalog** | http://localhost:8181 | Metadata management |
| **DuckDB CLI** | `just query-iceberg <table>` | High-performance analytics |

## ğŸ“‹ Commands

### **Main Workflow**
```bash
just full-workflow          # Complete CSV â†’ Iceberg pipeline
just csv-to-parquet         # Convert CSV files to Parquet
just create-iceberg-tables  # Create Iceberg tables with schema inspection
```

### **Service Management**
```bash
just start-services         # Start Trino + Iceberg catalog
just stop-services          # Stop all services
just status-services        # Check service health
just logs [service]         # View service logs
```

### **Data Querying**
```bash
just trino-cli              # Interactive Trino SQL session
just query-trino "SQL"      # Run single SQL query via Trino
just query-iceberg <table>  # Query table via DuckDB
just query-parquet <file> "SQL"  # Query Parquet directly
```

### **Data Inspection**
```bash
just list-data              # Show all available data files
just describe-iceberg <table>  # Show table schema
```

## ğŸ”§ Installation

### **Prerequisites**
- **Go 1.19+** (for building applications)
- **Docker** (for Trino + Iceberg services)
- **DuckDB** (optional, for direct querying)

### **Setup**
```bash
# Install dependencies
just deps

# Create data directories
just setup-data

# Build applications (optional)
just build
```

## ğŸ“ Directory Structure

```
the-modern-data-stack/
â”œâ”€â”€ cmd/
â”‚   â”œâ”€â”€ csv_to_parquet/         # CSV â†’ Parquet converter
â”‚   â””â”€â”€ create_iceberg_tables/  # Iceberg table creator
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ source/                 # Your CSV files (add here)
â”‚   â”œâ”€â”€ parquet/                # Generated Parquet files
â”‚   â””â”€â”€ iceberg_warehouse/      # Iceberg table storage
â”œâ”€â”€ etc/catalog/                # Trino catalog configurations
â”œâ”€â”€ docker-compose.yml          # Service orchestration
â”œâ”€â”€ justfile                    # Task automation
â””â”€â”€ README.md
```

## ğŸ’¡ Example Usage

### **Real Estate Data Analysis**

```bash
# 1. Add your data
cp transactions.csv data/source/
cp property_prices.csv data/source/

# 2. Process the data
just full-workflow

# 3. Analyze via Trino
just trino-cli
```

Then in Trino:
```sql
USE iceberg.my_data;
SHOW TABLES;
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run `just fmt` and `just test`
5. Submit a pull request

## ğŸ“š Learn More

- **Apache Iceberg**: https://iceberg.apache.org/
- **Trino**: https://trino.io/
- **DuckDB**: https://duckdb.org/
- **Docker Compose**: https://docs.docker.com/compose/

---

**Built with â¤ï¸ for modern data teams**
